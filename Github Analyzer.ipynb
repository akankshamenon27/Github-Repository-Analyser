{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "pip install torch\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from flask import Flask, request, render_template\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "MAX_FILE_SIZE = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetching User's Repositories from GitHub\n",
    "def fetch_user_repositories(username):\n",
    "    api_url = 'https://api.github.com/users/'+username+'/repos'\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        repositories = json.loads(response.text)\n",
    "        return repositories\n",
    "    else:\n",
    "        print(\"Failed to fetch repositories.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_user_repositories('vercel-labs')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing Code in Repositories\n",
    "def preprocess_code(repositories):\n",
    "    # Split large Jupyter notebooks or package files\n",
    "    for repository in repositories:\n",
    "        # Split large Jupyter notebooks or package files\n",
    "        if repository.get(\"file_type\") == \"Jupyter Notebook\" or repository.get(\"file_type\") == \"Package\":\n",
    "            files = repository.get(\"files\")\n",
    "            if files is not None:\n",
    "                repository[\"files\"] = split_large_files(files)\n",
    "\n",
    "    return repositories\n",
    "\n",
    "# Split large files into smaller parts\n",
    "def split_large_files(repository):\n",
    "    split_files = []\n",
    "\n",
    "    for file in repository.files:\n",
    "        if file.size > MAX_FILE_SIZE:  # Define the maximum file size threshold\n",
    "            chunks = split_file_into_chunks(file)\n",
    "            split_files.extend(chunks)\n",
    "        else:\n",
    "            split_files.append(file)\n",
    "\n",
    "    return split_files\n",
    "\n",
    "# Split a file into smaller chunks\n",
    "def split_file_into_chunks(file):\n",
    "    MAX_CHUNK_SIZE = 50000  # Maximum chunk size threshold in bytes\n",
    "\n",
    "    chunks = []\n",
    "    content = file.code\n",
    "    num_chunks = (len(content) // MAX_CHUNK_SIZE) + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * MAX_CHUNK_SIZE\n",
    "        end_idx = (i + 1) * MAX_CHUNK_SIZE\n",
    "        chunk_code = content[start_idx:end_idx]\n",
    "        chunk = File(file.name, file.file_type, chunk_code)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Identify Technical Complexity Using GPT and LangChain\n",
    "def assess_technical_complexity(repository):\n",
    "    preprocess_code(repository)\n",
    "    complexity_score = 0\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    \n",
    "    def process_repo(repository):\n",
    "        prompt = f'Evaluate the technical complexity of the {repository}. Please analyze the code and provide insights on its complexity, potential issues, and areas for improvement.'\n",
    "        input_ids = tokenizer.encode(prompt,truncation=True, max_length=100,return_tensors=\"pt\")\n",
    "        output = model.generate(input_ids, max_length=200,pad_token_id=tokenizer.eos_token_id)  # Example GPT-based evaluation\n",
    "        process_repo = tokenizer.decode(output[0])\n",
    "        return process_repo\n",
    "           \n",
    "    for file in repository:\n",
    "        processed_repo = process_repo(file)\n",
    "        complexity_score += len(processed_repo)\n",
    "\n",
    "    return complexity_score\n",
    "    \n",
    "def find_most_complex_repository(repositories):\n",
    "        most_complex_repository = None\n",
    "        highest_complexity_score = 0\n",
    "\n",
    "        for repository in repositories:\n",
    "            complexity_score = assess_technical_complexity(repository)\n",
    "\n",
    "            if complexity_score > highest_complexity_score:\n",
    "                highest_complexity_score = complexity_score\n",
    "                most_complex_repository = repository\n",
    "\n",
    "        return most_complex_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_technical_complexity(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
